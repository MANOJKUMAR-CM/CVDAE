{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c8691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import thop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea7d378",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f45d0",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_complex_data(x):\n",
    "    norm = torch.linalg.norm(x, dim=-2, keepdim=True) # Normalizing along columns\n",
    "    return x / (norm + 1e-8), norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02836307",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.from_numpy(np.load(\"../Dataset/speed-28-delayangle-32.npy\"))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9603e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:700] # first 700 samples; each sample contains 64 correlated frames; each frame is 8 x 76\n",
    "test_data = data[700:]\n",
    "\n",
    "print(f\"Training data: {train_data.shape}\")\n",
    "print(f\"Testing data: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c67e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.stack([train_data.real, train_data.imag], dim=2)\n",
    "test_data = torch.stack([test_data.real, test_data.imag], dim=2)\n",
    "\n",
    "print(\"Train shape:\", train_data.shape, train_data.dtype)\n",
    "print(\"Test shape:\", test_data.shape, test_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a5ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_norm, train_norm = normalize_complex_data(train_data)\n",
    "test_data_norm, test_norm = normalize_complex_data(test_data)\n",
    "train_data_norm.shape, train_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f2ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlockDecoded(nn.Module):\n",
    "    def __init__(self, channels=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, 8, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(16, channels, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(channels)\n",
    "        self.act3 = nn.LeakyReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        \n",
    "        y = self.conv1(x)\n",
    "        y = self.bn1(y)\n",
    "        y = self.act1(y)\n",
    "        \n",
    "        y = self.conv2(y)\n",
    "        y = self.bn2(y)\n",
    "        y = self.act2(y)\n",
    "        \n",
    "        y = self.conv3(y)\n",
    "        y = self.bn3(y)\n",
    "        \n",
    "        y = y + shortcut\n",
    "        y = self.act3(y)\n",
    "        return y\n",
    "\n",
    "class ResidualNetwork(nn.Module):\n",
    "    def __init__(self, img_channels=2, img_height=32, img_width=32, residual_num=1, encoded_dim=None):\n",
    "        super().__init__()\n",
    "        self.img_channels = img_channels\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.img_total = img_channels * img_height * img_width\n",
    "        \n",
    "        # 50% compression if encoded_dim not specified\n",
    "        if encoded_dim is None:\n",
    "            encoded_dim = self.img_total // 2\n",
    "        \n",
    "        self.conv_in = nn.Conv2d(img_channels, img_channels, kernel_size=3, padding=1)\n",
    "        self.bn_in = nn.BatchNorm2d(img_channels)\n",
    "        self.act_in = nn.LeakyReLU()\n",
    "        \n",
    "        self.encoder_fc = nn.Linear(self.img_total, 20)\n",
    "        self.decoder_fc = nn.Linear(20, self.img_total)\n",
    "        \n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlockDecoded(img_channels) for _ in range(residual_num)]\n",
    "        )\n",
    "        \n",
    "        self.conv_out = nn.Conv2d(img_channels, img_channels, kernel_size=3, padding=1)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_in(x)\n",
    "        x = self.bn_in(x)\n",
    "        x = self.act_in(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        \n",
    "        encoded = self.encoder_fc(x)\n",
    "        \n",
    "        x = self.decoder_fc(encoded)\n",
    "        x = x.view(x.size(0), self.img_channels, self.img_height, self.img_width)\n",
    "        \n",
    "        x = self.residual_blocks(x)\n",
    "        \n",
    "        x = self.conv_out(x)\n",
    "        # x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ba0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ResidualNetwork(img_channels=2, img_height=32, img_width=32, residual_num=1)\n",
    "model = model.to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572c8078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLOps computation\n",
    "d_input = torch.randn([1, 2, 32, 32]).to(device)\n",
    "flops, params = thop.profile(model, inputs=(d_input,), verbose=False)\n",
    "flops, params = thop.clever_format([flops, params], \"%.3f\")\n",
    "flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541ec1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.75e-3)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bdf3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=25\n",
    "best_loss = float('inf')\n",
    "model_path = \"../Models-100/CSINet-99.pth\"\n",
    "\n",
    "for i in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_data_norm:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        recons_output = model(batch)\n",
    "\n",
    "        loss = criterion(recons_output, batch)\n",
    "        epoch_loss+= loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "    # ---- checkpointing ----\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(model.state_dict(), model_path)  # save only parameters\n",
    "        print(f\"Epoch {i}, Loss {epoch_loss:.4f} New best model saved\")\n",
    "    else:\n",
    "        scheduler.step(epoch_loss)\n",
    "        print(f\"Epoch {i}, Loss {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7a5ef",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9ede52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = ResidualNetwork(img_channels=2, img_height=8, img_width=76, residual_num=1).to(device)  # rebuild same architecture\n",
    "# best_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# model = best_model\n",
    "# model_path = \"../Models/CSINet-80.pth\"\n",
    "# Load the polluted state_dict\n",
    "polluted_state_dict = torch.load(model_path)\n",
    "\n",
    "# Filter out keys with 'total_ops' or 'total_params'\n",
    "clean_state_dict = {k: v for k, v in polluted_state_dict.items() if \"total_ops\" not in k and \"total_params\" not in k}\n",
    "\n",
    "# Load only the clean keys\n",
    "model = ResidualNetwork(img_channels=2, img_height=32, img_width=32, residual_num=1).to(device)\n",
    "model.load_state_dict(clean_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01831df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(A, A_hat):\n",
    "    \n",
    "    batch_size = A.shape[0]\n",
    "    A = A.cpu().detach().numpy()\n",
    "    A_hat = A_hat.cpu().detach().numpy()\n",
    "    \n",
    "    corr = np.zeros(batch_size)\n",
    "    for i in range(batch_size):\n",
    "        In = A[i]\n",
    "        Out = A_hat[i]\n",
    "        \n",
    "        l = []\n",
    "        for j in range(32):\n",
    "            n1 = np.sqrt(np.sum(np.conj(In[:, j])*In[:, j]))\n",
    "            n2 = np.sqrt(np.sum(np.conj(Out[:, j])*Out[:, j]))\n",
    "    \n",
    "            num = np.abs(np.sum(np.conj(In[:, j])* Out[:, j]))\n",
    "\n",
    "            l.append(num/ (n1*n2 + 1e-12))\n",
    "            \n",
    "        corr[i] = np.mean(l)\n",
    "\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59631ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_NMSE(gt, pred):\n",
    "    \"\"\"\n",
    "    Compute NMSE in dB between complex-valued ground truth and prediction.\n",
    "\n",
    "    Args:\n",
    "        gt: Ground truth tensor of shape [B, 2, H, W], normalized [0, 1]\n",
    "        pred: Predicted tensor of same shape [B, 2, H, W]\n",
    "\n",
    "    Returns:\n",
    "        NMSE in dB (lower is better)\n",
    "    \"\"\"\n",
    "    \n",
    "    # De-centralize\n",
    "    gt = gt - 0.5\n",
    "    pred = pred - 0.5\n",
    "\n",
    "    # Compute power of complex ground truth\n",
    "    power_gt = gt[:, 0, :, :]**2 + gt[:, 1, :, :]**2\n",
    "\n",
    "    # Compute squared error\n",
    "    diff = gt - pred\n",
    "    mse = diff[:, 0, :, :]**2 + diff[:, 1, :, :]**2\n",
    "\n",
    "    # NMSE per sample\n",
    "    nmse = mse.sum(dim=[1, 2]) / (power_gt.sum(dim=[1, 2]) + 1e-12)\n",
    "\n",
    "    # Mean NMSE across batch and convert to dB\n",
    "    nmse_db = 10 * torch.log10(nmse.mean())\n",
    "\n",
    "    return nmse_db.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c14288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "model.eval()   # put model in eval mode\n",
    "test_loss = 0.0\n",
    "cosine_sim_col = []\n",
    "model_output = []\n",
    "nmse_db_total_test = 0.0\n",
    "\n",
    "with torch.no_grad():   # disable gradient computation\n",
    "    for batch in test_data_norm:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        recons_seq = model(batch)\n",
    "\n",
    "        # Accumulate NMSE in dB\n",
    "        nmse_db = compute_NMSE(batch, recons_seq)\n",
    "        nmse_db_total_test += nmse_db\n",
    "        \n",
    "        # Converting it to complex again\n",
    "        batch = torch.complex(batch[:, 0, :, :], batch[:, 1, :, :])\n",
    "        recons_seq = torch.complex(recons_seq[:, 0, :, :], recons_seq[:, 1, :, :])\n",
    "\n",
    "        model_output.append(recons_seq)\n",
    "\n",
    "        corr = correlation(batch, recons_seq)\n",
    "\n",
    "        cosine_sim_col.append(np.mean(corr))\n",
    "\n",
    "        loss = (criterion(recons_seq.real, batch.real) +\n",
    "                criterion(recons_seq.imag, batch.imag))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Cosine Similarity along columns Test Data (Normalized Data): {np.mean(cosine_sim_col):.4f}\")\n",
    "\n",
    "# Final average NMSE in dB\n",
    "avg_nmse_db = nmse_db_total_test / len(test_data_norm)\n",
    "print(f\"Average NMSE (dB) (Normalized Data): {avg_nmse_db:.2f} dB\")\n",
    "\n",
    "m_output = torch.stack(model_output)\n",
    "m_output = torch.stack([m_output.real, m_output.imag], dim=2)\n",
    "\n",
    "m_output = m_output.cpu() * test_norm # Multiplying the constants back\n",
    "\n",
    "nmse_db_total_test = 0.0\n",
    "\n",
    "for i in range(test_data.shape[0]):\n",
    "    nmse = compute_NMSE(test_data[i], m_output[i])\n",
    "    nmse_db_total_test += nmse\n",
    "\n",
    "\n",
    "\n",
    "m_output = torch.complex(m_output[:, :, 0, :, :], m_output[:, :, 1, :, :])\n",
    "\n",
    "test_data = torch.complex(test_data[:, :, 0, :, :], test_data[:, :, 1, :, :])\n",
    "\n",
    "cosine_sim_col = []\n",
    "for i in range(test_data.shape[0]):\n",
    "    corr = correlation(test_data[i], m_output[i])\n",
    "    cosine_sim_col.append(np.mean(corr))\n",
    "\n",
    "print(f\"Cosine Similarity along columns Test Data (ReNormalized Data): {np.mean(cosine_sim_col):.4f}\")\n",
    "\n",
    "# Final average NMSE in dB\n",
    "avg_nmse_db = nmse_db_total_test / len(test_data)\n",
    "print(f\"Average NMSE (dB) (ReNormalized Data): {avg_nmse_db:.2f} dB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5781bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "\n",
    "model.eval()   # put model in eval mode\n",
    "train_loss = 0.0\n",
    "cosine_sim_col = []\n",
    "model_output = []\n",
    "nmse_db_total = 0\n",
    "\n",
    "with torch.no_grad():   # disable gradient computation\n",
    "    for batch in train_data_norm:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        recons_seq = model(batch)\n",
    "\n",
    "        # Accumulate NMSE in dB\n",
    "        nmse_db = compute_NMSE(batch, recons_seq)\n",
    "        nmse_db_total += nmse_db\n",
    "        \n",
    "        # Converting it to complex again\n",
    "        batch = torch.complex(batch[:, 0, :, :], batch[:, 1, :, :])\n",
    "        recons_seq = torch.complex(recons_seq[:, 0, :, :], recons_seq[:, 1, :, :])\n",
    "\n",
    "        model_output.append(recons_seq)\n",
    "\n",
    "        corr = correlation(batch, recons_seq)\n",
    "\n",
    "        cosine_sim_col.append(np.mean(corr))\n",
    "\n",
    "        loss = (criterion(recons_seq.real, batch.real) +\n",
    "                criterion(recons_seq.imag, batch.imag))\n",
    "        train_loss += loss.item()\n",
    "\n",
    "print(f\"Train Loss: {train_loss:.4f}\")\n",
    "print(f\"Cosine Similarity along columns Train Data: {np.mean(cosine_sim_col):.4f}\")\n",
    "\n",
    "# Final average NMSE in dB\n",
    "avg_nmse_db = nmse_db_total / len(train_data_norm)\n",
    "\n",
    "print(f\"Average NMSE (dB): {avg_nmse_db:.2f} dB\")\n",
    "\n",
    "\n",
    "\n",
    "m_output = torch.stack(model_output)\n",
    "m_output = torch.stack([m_output.real, m_output.imag], dim=2)\n",
    "m_output = m_output.cpu() * train_norm # Multiplying the constants back\n",
    "\n",
    "nmse_db_total_test = 0.0\n",
    "\n",
    "for i in range(train_data.shape[0]):\n",
    "    nmse = compute_NMSE(train_data[i], m_output[i])\n",
    "    nmse_db_total_test += nmse\n",
    "\n",
    "m_output = torch.complex(m_output[:, :, 0, :, :], m_output[:, :, 1, :, :])\n",
    "\n",
    "train_data = torch.complex(train_data[:, :, 0, :, :], train_data[:, :, 1, :, :])\n",
    "\n",
    "cosine_sim_col = []\n",
    "for i in range(train_data.shape[0]):\n",
    "    corr = correlation(train_data[i], m_output[i])\n",
    "    cosine_sim_col.append(np.mean(corr))\n",
    "\n",
    "print(f\"Cosine Similarity along columns Train Data (ReNormalized Data): {np.mean(cosine_sim_col):.4f}\")\n",
    "\n",
    "# Final average NMSE in dB\n",
    "avg_nmse_db = nmse_db_total_test / len(train_data)\n",
    "print(f\"Average NMSE (dB) (ReNormalized Data): {avg_nmse_db:.2f} dB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae830547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
