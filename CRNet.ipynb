{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b7f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import thop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22726757",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57c1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_complex_data(x):\n",
    "    norm = torch.linalg.norm(x, dim=-2, keepdim=True) # Normalizing along columns\n",
    "    return x / (norm + 1e-8), norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.from_numpy(np.load(\"../Dataset/speed-28-delayangle-32.npy\"))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:700] # first 700 samples; each sample contains 64 correlated frames; each frame is 8 x 76\n",
    "test_data = data[700:]\n",
    "\n",
    "print(f\"Training data: {train_data.shape}\")\n",
    "print(f\"Testing data: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d0e4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.stack([train_data.real, train_data.imag], dim=2)\n",
    "test_data = torch.stack([test_data.real, test_data.imag], dim=2)\n",
    "\n",
    "print(\"Train shape:\", train_data.shape, train_data.dtype)\n",
    "print(\"Test shape:\", test_data.shape, test_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6768427",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_norm, train_norm = normalize_complex_data(train_data)\n",
    "test_data_norm, test_norm = normalize_complex_data(test_data)\n",
    "train_data_norm.shape, train_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43849af8",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a21ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRNET Model\n",
    "class ConvBN(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, groups=1):\n",
    "        if not isinstance(kernel_size, int):\n",
    "            padding = [(i - 1) // 2 for i in kernel_size]\n",
    "        else:\n",
    "            padding = (kernel_size - 1) // 2\n",
    "        super(ConvBN, self).__init__(OrderedDict([\n",
    "            ('conv', nn.Conv2d(in_planes, out_planes, kernel_size, stride,\n",
    "                               padding=padding, groups=groups, bias=False)),\n",
    "            ('bn', nn.BatchNorm2d(out_planes))\n",
    "        ]))\n",
    "\n",
    "\n",
    "class CRBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CRBlock, self).__init__()\n",
    "        self.path1 = nn.Sequential(OrderedDict([\n",
    "            ('conv3x3', ConvBN(2, 7, 3)),\n",
    "            ('relu1', nn.LeakyReLU(negative_slope=0.3, inplace=True)),\n",
    "            ('conv1x9', ConvBN(7, 7, [1, 9])),\n",
    "            ('relu2', nn.LeakyReLU(negative_slope=0.3, inplace=True)),\n",
    "            ('conv9x1', ConvBN(7, 7, [9, 1])),\n",
    "        ]))\n",
    "        self.path2 = nn.Sequential(OrderedDict([\n",
    "            ('conv1x5', ConvBN(2, 7, [1, 5])),\n",
    "            ('relu', nn.LeakyReLU(negative_slope=0.3, inplace=True)),\n",
    "            ('conv5x1', ConvBN(7, 7, [5, 1])),\n",
    "        ]))\n",
    "        self.conv1x1 = ConvBN(7 * 2, 2, 1)\n",
    "        self.identity = nn.Identity()\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.3, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.identity(x)\n",
    "\n",
    "        out1 = self.path1(x)\n",
    "        out2 = self.path2(x)\n",
    "        out = torch.cat((out1, out2), dim=1)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1x1(out)\n",
    "\n",
    "        out = self.relu(out + identity)\n",
    "        return out\n",
    "class CRNet(nn.Module):\n",
    "    def __init__(self, reduction=4, input_size=(32,32)):\n",
    "        super(CRNet, self).__init__()\n",
    "        in_channel = 2\n",
    "\n",
    "        self.encoder1 = nn.Sequential(OrderedDict([\n",
    "            (\"conv3x3_bn\", ConvBN(in_channel, 2, 3)),\n",
    "            (\"relu1\", nn.LeakyReLU(negative_slope=0.3, inplace=True)),\n",
    "            (\"conv1x9_bn\", ConvBN(2, 2, [1, 9])),\n",
    "            (\"relu2\", nn.LeakyReLU(negative_slope=0.3, inplace=True)),\n",
    "            (\"conv9x1_bn\", ConvBN(2, 2, [9, 1])),\n",
    "        ]))\n",
    "        self.encoder2 = ConvBN(in_channel, 2, 3)\n",
    "        self.encoder_conv = nn.Sequential(OrderedDict([\n",
    "            (\"relu1\", nn.LeakyReLU(negative_slope=0.3, inplace=True)),\n",
    "            (\"conv1x1_bn\", ConvBN(4, 2, 1)),\n",
    "            (\"relu2\", nn.LeakyReLU(negative_slope=0.3, inplace=True)),\n",
    "        ]))\n",
    "\n",
    "        self.total_size = self._get_flattened_size(in_channel, input_size)\n",
    "\n",
    "        self.encoder_fc = nn.Linear(self.total_size, self.total_size // reduction)\n",
    "        self.decoder_fc = nn.Linear(self.total_size // reduction, self.total_size)\n",
    "\n",
    "        decoder = OrderedDict([\n",
    "            (\"conv5x5_bn\", ConvBN(2, 2, 5)),\n",
    "            (\"relu\", nn.LeakyReLU(negative_slope=0.3, inplace=True)),\n",
    "            (\"CRBlock1\", CRBlock()),\n",
    "            (\"CRBlock2\", CRBlock())\n",
    "        ])\n",
    "        self.decoder_feature = nn.Sequential(decoder)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Initialize weights (your original init code)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _get_flattened_size(self, in_channel, input_size):\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, in_channel, *input_size)\n",
    "            encode1 = self.encoder1(x)\n",
    "            encode2 = self.encoder2(x)\n",
    "            out = torch.cat((encode1, encode2), dim=1)\n",
    "            out = self.encoder_conv(out)\n",
    "            flattened_size = out.view(1, -1).size(1)\n",
    "            # print(flattened_size)\n",
    "        return flattened_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, c, h, w = x.size()\n",
    "        encode1 = self.encoder1(x)\n",
    "        encode2 = self.encoder2(x)\n",
    "        out = torch.cat((encode1, encode2), dim=1)\n",
    "        out = self.encoder_conv(out)\n",
    "        out = self.encoder_fc(out.view(n, -1))\n",
    "        out = self.decoder_fc(out).view(n, c, h, w)\n",
    "        out = self.decoder_feature(out)\n",
    "        # out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def crnet(reduction=2):\n",
    "    r\"\"\" Create a proposed CRNet.\n",
    "\n",
    "    :param reduction: the reciprocal of compression ratio\n",
    "    :return: an instance of CRNet\n",
    "    \"\"\"\n",
    "\n",
    "    model = CRNet(reduction=reduction)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f35880",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = crnet(100).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e48584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FlOps computation\n",
    "d_input = torch.randn([1, 2, 32, 32]).to(device)\n",
    "flops, params = thop.profile(model, inputs=(d_input,), verbose=False)\n",
    "flops, params = thop.clever_format([flops, params], \"%.3f\")\n",
    "flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70326014",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-3)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4530baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs=25\n",
    "best_loss = float('inf')\n",
    "model_path = \"../Models-100/CRNet-99.pth\"\n",
    "\n",
    "for i in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_data_norm:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        recons_output = model(batch)\n",
    "\n",
    "        loss = criterion(recons_output, batch)\n",
    "        epoch_loss+= loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "    # ---- checkpointing ----\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(model.state_dict(), model_path)  # save only parameters\n",
    "        print(f\"Epoch {i}, Loss {epoch_loss:.4f} New best model saved\")\n",
    "    else:\n",
    "        scheduler.step(epoch_loss)\n",
    "        print(f\"Epoch {i}, Loss {epoch_loss:.4f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a7078",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd5dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "# model_path = \"CRNet-99.pth\"\n",
    "# best_model = crnet(100).to(device)  # rebuild same architecture\n",
    "# best_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# model = best_model\n",
    "\n",
    "# Load the polluted state_dict\n",
    "polluted_state_dict = torch.load(model_path)\n",
    "\n",
    "# Filter out keys with 'total_ops' or 'total_params'\n",
    "clean_state_dict = {k: v for k, v in polluted_state_dict.items() if \"total_ops\" not in k and \"total_params\" not in k}\n",
    "\n",
    "# Load only the clean keys\n",
    "model = crnet(100).to(device)\n",
    "model.load_state_dict(clean_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1384e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(A, A_hat):\n",
    "    \n",
    "    batch_size = A.shape[0]\n",
    "    A = A.cpu().detach().numpy()\n",
    "    A_hat = A_hat.cpu().detach().numpy()\n",
    "    \n",
    "    corr = np.zeros(batch_size)\n",
    "    for i in range(batch_size):\n",
    "        In = A[i]\n",
    "        Out = A_hat[i]\n",
    "        \n",
    "        l = []\n",
    "        for j in range(32):\n",
    "            n1 = np.sqrt(np.sum(np.conj(In[:, j])*In[:, j]))\n",
    "            n2 = np.sqrt(np.sum(np.conj(Out[:, j])*Out[:, j]))\n",
    "    \n",
    "            num = np.abs(np.sum(np.conj(In[:, j])* Out[:, j]))\n",
    "\n",
    "            l.append(num/ (n1*n2 + 1e-12))\n",
    "            \n",
    "        corr[i] = np.mean(l)\n",
    "\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba7ca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_NMSE(gt, pred):\n",
    "    \"\"\"\n",
    "    Compute NMSE in dB between complex-valued ground truth and prediction.\n",
    "\n",
    "    Args:\n",
    "        gt: Ground truth tensor of shape [B, 2, H, W], normalized [0, 1]\n",
    "        pred: Predicted tensor of same shape [B, 2, H, W]\n",
    "\n",
    "    Returns:\n",
    "        NMSE in dB (lower is better)\n",
    "    \"\"\"\n",
    "    \n",
    "    # De-centralize\n",
    "    gt = gt - 0.5\n",
    "    pred = pred - 0.5\n",
    "\n",
    "    # Compute power of complex ground truth\n",
    "    power_gt = gt[:, 0, :, :]**2 + gt[:, 1, :, :]**2\n",
    "\n",
    "    # Compute squared error\n",
    "    diff = gt - pred\n",
    "    mse = diff[:, 0, :, :]**2 + diff[:, 1, :, :]**2\n",
    "\n",
    "    # NMSE per sample\n",
    "    nmse = mse.sum(dim=[1, 2]) / (power_gt.sum(dim=[1, 2]) + 1e-12)\n",
    "\n",
    "    # Mean NMSE across batch and convert to dB\n",
    "    nmse_db = 10 * torch.log10(nmse.mean())\n",
    "\n",
    "    return nmse_db.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19121dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "model.eval()   # put model in eval mode\n",
    "test_loss = 0.0\n",
    "cosine_sim_col = []\n",
    "model_output = []\n",
    "nmse_db_total_test = 0.0\n",
    "\n",
    "with torch.no_grad():   # disable gradient computation\n",
    "    for batch in test_data_norm:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        recons_seq = model(batch)\n",
    "\n",
    "        # Accumulate NMSE in dB\n",
    "        nmse_db = compute_NMSE(batch, recons_seq)\n",
    "        nmse_db_total_test += nmse_db\n",
    "        \n",
    "        # Converting it to complex again\n",
    "        batch = torch.complex(batch[:, 0, :, :], batch[:, 1, :, :])\n",
    "        recons_seq = torch.complex(recons_seq[:, 0, :, :], recons_seq[:, 1, :, :])\n",
    "\n",
    "        model_output.append(recons_seq)\n",
    "\n",
    "        corr = correlation(batch, recons_seq)\n",
    "\n",
    "        cosine_sim_col.append(np.mean(corr))\n",
    "\n",
    "        loss = (criterion(recons_seq.real, batch.real) +\n",
    "                criterion(recons_seq.imag, batch.imag))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Cosine Similarity along columns Test Data (Normalized Data): {np.mean(cosine_sim_col):.4f}\")\n",
    "\n",
    "# Final average NMSE in dB\n",
    "avg_nmse_db = nmse_db_total_test / len(test_data_norm)\n",
    "print(f\"Average NMSE (dB) (Normalized Data): {avg_nmse_db:.2f} dB\")\n",
    "\n",
    "\n",
    "m_output = torch.stack(model_output)\n",
    "m_output = torch.stack([m_output.real, m_output.imag], dim=2)\n",
    "\n",
    "m_output = m_output.cpu() * test_norm # Multiplying the constants back\n",
    "\n",
    "nmse_db_total_test = 0.0\n",
    "\n",
    "for i in range(test_data.shape[0]):\n",
    "    nmse = compute_NMSE(test_data[i], m_output[i])\n",
    "    nmse_db_total_test += nmse\n",
    "\n",
    "\n",
    "m_output = torch.complex(m_output[:, :, 0, :, :], m_output[:, :, 1, :, :])\n",
    "test_data = torch.complex(test_data[:, :, 0, :, :], test_data[:, :, 1, :, :])\n",
    "\n",
    "cosine_sim_col = []\n",
    "for i in range(test_data.shape[0]):\n",
    "    corr = correlation(test_data[i], m_output[i])\n",
    "    cosine_sim_col.append(np.mean(corr))\n",
    "\n",
    "print(f\"Cosine Similarity along columns Test Data (ReNormalized Data): {np.mean(cosine_sim_col):.4f}\")\n",
    "# Final average NMSE in dB\n",
    "avg_nmse_db = nmse_db_total_test / len(test_data)\n",
    "print(f\"Average NMSE (dB) (ReNormalized Data): {avg_nmse_db:.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c1fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "\n",
    "model.eval()   # put model in eval mode\n",
    "train_loss = 0.0\n",
    "cosine_sim_col = []\n",
    "model_output = []\n",
    "nmse_db_total = 0\n",
    "\n",
    "with torch.no_grad():   # disable gradient computation\n",
    "    for batch in train_data_norm:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        recons_seq = model(batch)\n",
    "\n",
    "        # Accumulate NMSE in dB\n",
    "        nmse_db = compute_NMSE(batch, recons_seq)\n",
    "        nmse_db_total += nmse_db\n",
    "        \n",
    "        # Converting it to complex again\n",
    "        batch = torch.complex(batch[:, 0, :, :], batch[:, 1, :, :])\n",
    "        recons_seq = torch.complex(recons_seq[:, 0, :, :], recons_seq[:, 1, :, :])\n",
    "\n",
    "        model_output.append(recons_seq)\n",
    "\n",
    "        corr = correlation(batch, recons_seq)\n",
    "\n",
    "        cosine_sim_col.append(np.mean(corr))\n",
    "\n",
    "        loss = (criterion(recons_seq.real, batch.real) +\n",
    "                criterion(recons_seq.imag, batch.imag))\n",
    "        train_loss += loss.item()\n",
    "\n",
    "print(f\"Train Loss: {train_loss:.4f}\")\n",
    "print(f\"Cosine Similarity along columns Train Data: {np.mean(cosine_sim_col):.4f}\")\n",
    "\n",
    "# Final average NMSE in dB\n",
    "avg_nmse_db = nmse_db_total / len(train_data_norm)\n",
    "\n",
    "print(f\"Average NMSE (dB): {avg_nmse_db:.2f} dB\")\n",
    "\n",
    "\n",
    "m_output = torch.stack(model_output)\n",
    "m_output = torch.stack([m_output.real, m_output.imag], dim=2)\n",
    "m_output = m_output.cpu() * train_norm # Multiplying the constants back\n",
    "\n",
    "nmse_db_total_test = 0.0\n",
    "\n",
    "for i in range(train_data.shape[0]):\n",
    "    nmse = compute_NMSE(train_data[i], m_output[i])\n",
    "    nmse_db_total_test += nmse\n",
    "\n",
    "\n",
    "m_output = torch.complex(m_output[:, :, 0, :, :], m_output[:, :, 1, :, :])\n",
    "\n",
    "train_data = torch.complex(train_data[:, :, 0, :, :], train_data[:, :, 1, :, :])\n",
    "\n",
    "cosine_sim_col = []\n",
    "for i in range(train_data.shape[0]):\n",
    "    corr = correlation(train_data[i], m_output[i])\n",
    "    cosine_sim_col.append(np.mean(corr))\n",
    "\n",
    "print(f\"Cosine Similarity along columns Train Data (ReNormalized Data): {np.mean(cosine_sim_col):.4f}\")\n",
    "\n",
    "# Final average NMSE in dB\n",
    "avg_nmse_db = nmse_db_total_test / len(train_data)\n",
    "print(f\"Average NMSE (dB) (ReNormalized Data): {avg_nmse_db:.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f41239d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d72f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
